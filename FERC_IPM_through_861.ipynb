{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gschivley/FERC_714/blob/master/FERC_IPM_through_861.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import zipfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import fuzzywuzzy\n",
    "\n",
    "pd.set_option(\"max_rows\", 50)\n",
    "\n",
    "cwd = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the FERC 714 data to a temp folder that google is nice enough to host\n",
    "# No need to run if already downloaded.\n",
    "url = 'https://www.ferc.gov/docs-filing/forms/form-714/data/form714-database.zip'\n",
    "save_folder = cwd / \"FERC\"\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "ferc_data_path = save_folder / \"form714-database\"\n",
    "if not ferc_data_path.exists():\n",
    "    urllib.request.urlretrieve(url, save_folder / 'form714-database.zip')\n",
    "\n",
    "    ### Unzip it\n",
    "    with zipfile.ZipFile(save_folder / 'form714-database.zip', 'r') as zfile:\n",
    "        zfile.extractall(ferc_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also download/extract the 2012 EIA 861 data.\n",
    "url = 'https://www.eia.gov/electricity/data/eia861/archive/zip/f8612012.zip'\n",
    "save_folder = cwd / \"EIA861\"\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "eia861_data_path = save_folder / \"f8612012\"\n",
    "if not eia861_data_path.exists():\n",
    "    urllib.request.urlretrieve(url, save_folder / 'f8612012.zip')\n",
    "    ### Unzip it\n",
    "    with zipfile.ZipFile(save_folder / 'f8612012.zip', 'r') as zfile:\n",
    "        zfile.extractall(eia861_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to find the best name match using tfidf and cosine similarity\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "def vectorize_tfidf(series1, series2):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "    \n",
    "    vectorizer.fit(pd.concat([series1, series2]))\n",
    "    tf_idf_matrix_1 = vectorizer.transform(series1)\n",
    "    tf_idf_matrix_2 = vectorizer.transform(series2)\n",
    "    \n",
    "    return tf_idf_matrix_1, tf_idf_matrix_2\n",
    "\n",
    "def top_name_match(ferc_df, eia_df):\n",
    "    ferc_series = ferc_df['respondent_name'].reset_index(drop=True)\n",
    "    eia_series = eia_df['eia_name'].reset_index(drop=True)\n",
    "    \n",
    "    tfidf1, tfidf2 = vectorize_tfidf(ferc_series, eia_series)\n",
    "    cos_sim = cosine_similarity(tfidf1, tfidf2)\n",
    "    \n",
    "    cols = [\n",
    "        'respondent_name', 'eia_name', 'score',\n",
    "    ]\n",
    "    results_df = pd.DataFrame(columns=cols, index=ferc_series.index)\n",
    "    results_df['respondent_name'] = ferc_series\n",
    "    matched_entities = []\n",
    "    for idx, name in ferc_series.iteritems():\n",
    "        best_match = np.argmax(cos_sim[idx])\n",
    "        score = cos_sim[idx].max()\n",
    "        eia_name = eia_series[best_match]\n",
    "        matched_entities.append(eia_name)\n",
    "        results_df.loc[idx, 'eia_name'] = eia_name\n",
    "        results_df.loc[idx, 'score'] = score\n",
    "        \n",
    "    results_df = results_df.merge(\n",
    "        ferc_df.loc[:, ['respondent_name', \"respondent_id\", \"eia_code\"]],\n",
    "        on='respondent_name', how='left'\n",
    "    )\n",
    "    \n",
    "    results_df = results_df.merge(\n",
    "        eia_df,\n",
    "        on='eia_name', how='left'\n",
    "    )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of respondents that report data in 2012\n",
    "\n",
    "ferc_714 = pd.read_csv(\n",
    "    ferc_data_path / \"Part 3 Schedule 2 - Planning Area Hourly Demand.csv\",\n",
    "    parse_dates=[\"plan_date\"], infer_datetime_format=True\n",
    ")\n",
    "\n",
    "valid_respondents = ferc_714.loc[ferc_714.report_yr == 2012, \"respondent_id\"].unique()\n",
    "len(sorted(valid_respondents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the FERC respondents\n",
    "This includes the FERC respondent ID and associated EIA code. These codes sometimes match utilities and sometimes match BAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes that I'm changing after manual investigation. Worth double checking.\n",
    "alt_ferc_eia_codes = {\n",
    "    272: 25470, # WAPA upper great plains east\n",
    "    125: 2775, # CAISO\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferc_respondents = pd.read_csv(\n",
    "    ferc_data_path / \"Respondent IDs.csv\"\n",
    ")\n",
    "ferc_respondents.loc[:, \"respondent_name\"] = ferc_respondents.loc[:, \"respondent_name\"].str.strip()\n",
    "ferc_respondents = ferc_respondents.loc[ferc_respondents[\"respondent_id\"].isin(valid_respondents), :]\n",
    "\n",
    "for ferc_id, eia_code in alt_ferc_eia_codes.items():\n",
    "    ferc_respondents.loc[\n",
    "        ferc_respondents[\"respondent_id\"] == ferc_id, \n",
    "        \"eia_code\"\n",
    "    ] = eia_code\n",
    "    \n",
    "# ferc_respondents_eia_map = ferc_respondents.set_index(\"respondent_id\")\n",
    "# ferc_respondents_eia_map = ferc_respondents_eia_map.drop(columns=\"respondent_name\")\n",
    "ferc_respondents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First match utilities\n",
    "Following the method described in the SI of [this PNAS paper (Auffhammer et al, 2017)](https://www.pnas.org/content/114/8/1886), first match FERC respondents to EIA utilities. I'm using 2012 EIA-861 data to match with the 2012 load data. Start with matching against the service territory data since we eventually need to be matching counties anyway.\n",
    "\n",
    "CAISO matches with the City of Albany (before the manual change I made above in `alt_ferc_eia_codes`) but all other utilities seem to match fine against respondents. A total of 71 respondents match with utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_data = pd.read_excel(eia861_data_path / \"utility_data_2012.xls\", skiprows=1)\n",
    "utility_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_territory = pd.read_excel(eia861_data_path / \"service_territory_2012.xls\")\n",
    "service_territory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_utilities = service_territory[[\"Utility Number\", \"Utility Name\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility = pd.merge(\n",
    "    ferc_respondents, \n",
    "    utility_data[[\"Utility Number\", \"Utility Name\"]], \n",
    "    left_on=\"eia_code\", \n",
    "    right_on=\"Utility Number\", \n",
    "    how=\"left\"\n",
    ")\n",
    "respondent_utility.loc[\n",
    "    respondent_utility[\"Utility Number\"].isin(st_utilities[\"Utility Number\"]),\n",
    "    \"geo_data_available\"\n",
    "] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility.notna().all(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility_sa = pd.merge(\n",
    "    ferc_respondents, \n",
    "    st_utilities, \n",
    "    left_on=\"eia_code\", \n",
    "    right_on=\"Utility Number\", \n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility_sa.notna().all(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fuzzywuzzy to get a quick score on the name matches\n",
    "def name_match_score(row, eia_col):\n",
    "    ferc_name = row[\"respondent_name\"]\n",
    "    utility_name = row[eia_col]\n",
    "    \n",
    "    if pd.isna(utility_name):\n",
    "        return np.nan\n",
    "    else:\n",
    "        score = fuzz.partial_token_sort_ratio(ferc_name, utility_name)\n",
    "        return score\n",
    "    \n",
    "respondent_utility_sa[\"name_score\"] = respondent_utility_sa.apply(\n",
    "    lambda row: name_match_score(row, \"Utility Name\"), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility[\"name_score\"] = respondent_utility.apply(\n",
    "    lambda row: name_match_score(row, \"Utility Name\"), axis=1\n",
    ")\n",
    "respondent_utility.sort_values(\"name_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_utility_sa.sort_values(\"name_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match remaining respondents against BAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_ba = pd.read_excel(eia861_data_path / \"balancing_authority_2012.xls\")\n",
    "\n",
    "eia_bas = utility_ba.loc[:, [\"BA Code\", \"Balancing Authority Name\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_utility_respondents = respondent_utility_sa.loc[\n",
    "    respondent_utility_sa.isnull().any(axis=1), \n",
    "    [\"respondent_id\", \"respondent_name\", \"eia_code\"]\n",
    "]\n",
    "respondent_ba = pd.merge(non_utility_respondents, eia_bas, left_on=\"eia_code\", right_on=\"BA Code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_utility_respondents = respondent_utility.loc[\n",
    "    respondent_utility.isnull().any(axis=1), \n",
    "    [\"respondent_id\", \"respondent_name\", \"eia_code\"]\n",
    "]\n",
    "respondent_ba = pd.merge(non_utility_respondents, eia_bas, left_on=\"eia_code\", right_on=\"BA Code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_ba[\"name_score\"] = respondent_ba.apply(\n",
    "    lambda row: name_match_score(row, \"Balancing Authority Name\"), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_ba.sort_values(\"name_score\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_ba.sort_values(\"name_score\").tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String match remaining respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_codes = utility_ba.loc[:, [\"BA Code\", \"Balancing Authority Name\"]].drop_duplicates()\n",
    "ba_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_respondents = respondent_ba.loc[\n",
    "    respondent_ba.isna().any(axis=1),\n",
    "    [\"respondent_id\", \"respondent_name\", \"eia_code\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of potential respondents with <eia code/number>: <name>\n",
    "# from all the potential input files.\n",
    "potential_resp_list = [\n",
    "    st_utilities.set_index(\"Utility Number\").to_dict()[\"Utility Name\"],\n",
    "    eia_bas.set_index(\"BA Code\").to_dict()[\"Balancing Authority Name\"],\n",
    "    utility_ba[[\"Utility Number\", \"Utility Name\"]].drop_duplicates().set_index(\"Utility Number\").to_dict()[\"Utility Name\"]\n",
    "]\n",
    "potential_resp_dict = {k: v for d in potential_resp_list for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best name match, then check to see if the EIA code also matches.\n",
    "# The fact that some codes match means that I didn't catch all utilities/BAs\n",
    "# in the sections above.\n",
    "\n",
    "name_match = top_name_match(remaining_respondents, pd.DataFrame(potential_resp_dict.items(), columns=[\"eia_id\", \"eia_name\"]))\n",
    "name_match[\"eia_code_match\"] = False\n",
    "name_match.loc[\n",
    "    name_match[\"eia_code\"] == name_match[\"eia_id\"],\n",
    "    \"eia_code_match\"\n",
    "] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_match.sort_values(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_match.query(\"eia_code_match==False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my preliminary list of string matches to keep\n",
    "use_name_match = [\n",
    "    \"Westar Energy (KPL)\",\n",
    "    \"PacifiCorp - Part II Sch 2 (East & West combined)\",\n",
    "    \"City of West Memphis\"\n",
    "]\n",
    "\n",
    "# Keep matches where the codes are the same or the names are in the list above.\n",
    "good_name_matches = name_match.loc[\n",
    "    (name_match[\"eia_code_match\"] == True)\n",
    "    | (name_match[\"respondent_name\"].isin(use_name_match)),\n",
    "    :\n",
    "].set_index(\"respondent_id\")\n",
    "good_name_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_ipm_match = {\n",
    "    128: \"S_VACA\", # Central Electric Power Cooperative, Inc. - located in SC\n",
    "#     292: \"S_SOU\", # Southern Power Company\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results and find all county matches\n",
    "\n",
    "TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "powergenome",
   "language": "python",
   "name": "powergenome"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
