{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gschivley/FERC_714/blob/master/FERC714_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_b2KbSpG2SW"
   },
   "source": [
    "# FERC 714 hourly demand data\n",
    "\n",
    "This notebook extracts a couple years of hourly demand data from FERC 714 and starts exploring ways to match the FERC respondents to EIA utility/balancing authority entities in EIA-861. My goal is to match the FERC respondents with IPM regions. It's a working document that I'm sharing with the hope that other people will be able to check and improve on what I've done. If you have questions or want to suggest changes/additional data you can [open an issue](https://github.com/gschivley/FERC_714/issues), find on on twitter ([@gschivley](https://twitter.com/gschivley)), or email me at *greg at carbonimpact dot co*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "bHLnKurAEF4i",
    "outputId": "de75bd7c-9766-44c3-a6c8-e93dd06a4b57"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/gschivley/EIA_Cleaned_Hourly_Electricity_Demand_Code/master/anomaly_screening.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdX3O0YiZuYx"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import urllib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from anomaly_screening import screen_timeseries, make_anomaly_summary\n",
    "\n",
    "cwd = Path.cwd()\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S71Ky-ibeJ6g"
   },
   "outputs": [],
   "source": [
    "# Download the FERC 714 data to a temp folder that google is nice enough to host.\n",
    "url = 'https://www.ferc.gov/docs-filing/forms/form-714/data/form714-database.zip'\n",
    "save_folder = cwd / \"FERC\"\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "urllib.request.urlretrieve(url, save_folder / 'form714-database.zip')\n",
    "### Unzip it\n",
    "data_path = save_folder / \"form714-database\"\n",
    "with zipfile.ZipFile(save_folder / 'form714-database.zip', 'r') as zfile:\n",
    "    zfile.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "yEwru4tCfgfS",
    "outputId": "9ea7894a-fe49-431e-d960-78b11d446680"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    data_path / \"Part 3 Schedule 2 - Planning Area Hourly Demand.csv\",\n",
    "    parse_dates=[\"plan_date\"], infer_datetime_format=True\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "jU9KXMvnfgN4",
    "outputId": "a0504a7b-17a8-42e2-a194-fa8de7126615"
   },
   "outputs": [],
   "source": [
    "respondents = df.loc[df.report_yr >=2011,\"respondent_id\"].unique()\n",
    "\n",
    "respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On5aCkQVfgD4"
   },
   "outputs": [],
   "source": [
    "def hour_25_looks_real(df):\n",
    "    \"\"\"\n",
    "    The FERC demand data has a column hour25 for daylight savings. Determine if\n",
    "    the value there looks like it could be a continuation of the series.\n",
    "    Sometimes it seems to be a sum of all values for the day or something else\n",
    "    weird.\n",
    "    \"\"\"\n",
    "    df[\"hour_25_valid\"] = False\n",
    "    df[\"hour24_25_ratio\"] = df[\"hour24\"] / df[\"hour25\"]\n",
    "    df.loc[\n",
    "        (df[\"hour24_25_ratio\"] > 0.6)\n",
    "        & (df[\"hour24_25_ratio\"] < 1.5),\n",
    "        \"hour_25_valid\"\n",
    "    ] = True\n",
    "    \n",
    "    df.loc[df[\"hour_25_valid\"] == False, \"hour25\"] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxf4Kh12ftBO"
   },
   "outputs": [],
   "source": [
    "# Only keeping 2011/2012 for now. Explore more of the data if you want!\n",
    "corrected_df = hour_25_looks_real(\n",
    "    df.loc[(df.report_yr.isin([2011, 2012])), :].copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "AVxgDpMyftOa",
    "outputId": "bd85def2-1884-4a27-a8f9-d506f35daec3"
   },
   "outputs": [],
   "source": [
    "# Check to see how many valid hour25 values there are for each respondent.\n",
    "# Looks like 311 uses hour25 all the time...\n",
    "hour_25_count = {}\n",
    "for r in corrected_df.dropna()[\"respondent_id\"].unique():\n",
    "    hour_25_count[r] = len(corrected_df.query(\"respondent_id==@r\").dropna())\n",
    "\n",
    "hour_25_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "colab_type": "code",
    "id": "ZDOBllPNftce",
    "outputId": "d2536bbe-0c66-4552-8971-4e45371e864d"
   },
   "outputs": [],
   "source": [
    "# hour25 values do appear to be on dayslight savings change\n",
    "corrected_df.query(\"hour25 > 0 & respondent_id != 311\").sort_values(\"report_yr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UO76JcZSg_EA",
    "outputId": "78d6bc44-7c9b-4d7e-ba33-dfb83b8c80eb"
   },
   "outputs": [],
   "source": [
    "# Same for a 0 value in hour02, which happens sometimes instead of skipping the hour\n",
    "corrected_df.query(\"hour02 == 0 & hour01 != 0\").sort_values(\"report_yr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DBvKfmZhJnu"
   },
   "outputs": [],
   "source": [
    "def timezone_to_tz(timezone):\n",
    "    return 'Etc/GMT{:+}'.format(-timezone)\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "6clbqBQ5hJcS",
    "outputId": "9179e1f6-9f65-4430-ebe6-a07bf712dd60"
   },
   "outputs": [],
   "source": [
    "hourcols = ['hour{:02.0f}'.format(i) for i in range(1,26)]\n",
    "\n",
    "# These are my best guesses for all of the timezone values that BAs listed\n",
    "tz_offset = {\n",
    "    '1  ': -5,\n",
    "    \"   \": -7,\n",
    "    \"AKS\": -9,\n",
    "    \"CST\": -6,\n",
    "    \"CPT\": -6,\n",
    "    \"MST\": -7,\n",
    "    \"PST\": -8,\n",
    "    \"PDT\": -8,\n",
    "    \"mst\": -7,\n",
    "    \"EST\": -5,\n",
    "    \"EDT\": -5,\n",
    "    \" CS\": -6,\n",
    "    \"HST\": -10,\n",
    "    \"Est\": -5,\n",
    "    \"PPT\": -8,\n",
    "    \"MPT\": -7,\n",
    "    \"EPT\": -5,  \n",
    "    \"MPP\": -7,\n",
    "}\n",
    "tz_ba = {key: timezone_to_tz(offset) for key, offset in tz_offset.items()}\n",
    "error_dfs = {}\n",
    "good_dfs = {}\n",
    "years = [2011, 2012]\n",
    "year_hours = {\n",
    "    2011: 8760,\n",
    "    2012: 8784\n",
    "}\n",
    "\n",
    "for r in respondents:\n",
    "    # Not all respondents have data for all years\n",
    "    r_all_years = corrected_df.loc[\n",
    "            (corrected_df.respondent_id == r) & (corrected_df.report_yr.isin(years)),\n",
    "            :\n",
    "        ]\n",
    "    # Only proceed if there is positive demand over all years (skip if all 0)\n",
    "    if r_all_years[hourcols].sum().sum() > 0:\n",
    "        valid_years = sorted(r_all_years[\"report_yr\"].unique().tolist())\n",
    "        tz = r_all_years[\"timezone\"].values[0]\n",
    "        dt = pd.date_range(\n",
    "            f\"{valid_years[0]}-01-01\", \n",
    "            f\"{valid_years[-1] + 1}-01-01\",\n",
    "            freq=\"H\",\n",
    "            closed=\"left\",\n",
    "            tz=tz_ba[tz]\n",
    "        )\n",
    "\n",
    "        df_list = []\n",
    "        for year in valid_years:\n",
    "            r_single_year = r_all_years.loc[r_all_years.report_yr == year, :]\n",
    "\n",
    "            # Try to drop March DST changeover values if there are more valid hours\n",
    "            # (not nan) than hours in the year.\n",
    "            # I found that hour02, hour03, and hour24 all had 0 values for at\n",
    "            # least one respondent.\n",
    "            # Set the 0 values to np.nan so they can be dropped after melting\n",
    "            if r_single_year[hourcols].count().sum() > year_hours[year]:\n",
    "                r_single_year.loc[\n",
    "                    (r_single_year[\"hour02\"] == 0)\n",
    "                    & (r_single_year[\"hour01\"] != 0)\n",
    "                    & (r_single_year[\"plan_date\"].dt.month == 3),\n",
    "                    \"hour02\"\n",
    "                ] = np.nan\n",
    "                r_single_year.loc[\n",
    "                    (r_single_year[\"hour03\"] == 0)\n",
    "                    & (r_single_year[\"hour01\"] != 0)\n",
    "                    & (r_single_year[\"plan_date\"].dt.month == 3),\n",
    "                    \"hour03\"\n",
    "                ] = np.nan\n",
    "                r_single_year.loc[\n",
    "                    (r_single_year[\"hour24\"] == 0)\n",
    "                    & (r_single_year[\"hour01\"] != 0)\n",
    "                    & (r_single_year[\"plan_date\"].dt.month == 3),\n",
    "                    \"hour24\"\n",
    "                ] = np.nan\n",
    "            \n",
    "            tidy_df = pd.melt(r_single_year, id_vars='plan_date', value_vars=hourcols, \n",
    "                 var_name='hour', value_name='demand_MW')\n",
    "            tidy_df = tidy_df.sort_values([\"plan_date\", \"hour\"])\n",
    "\n",
    "            tidy_df = tidy_df.dropna()\n",
    "            tidy_df[\"hour\"] = tidy_df[\"hour\"].str[-2:].astype(int)\n",
    "            tidy_df[\"respondent_id\"] = r\n",
    "\n",
    "            df_list.append(tidy_df)\n",
    "\n",
    "        # Concat the years together\n",
    "        r_df = pd.concat(df_list)\n",
    "        r_df = r_df.reset_index(drop=True)\n",
    "\n",
    "        # If the length of one year, all years, or some combination of years\n",
    "        if len(r_df) in [sum(x) for x in powerset(year_hours.values())]:\n",
    "\n",
    "            r_df[\"date_time\"] = dt\n",
    "            columns = [\"date_time\", \"demand_MW\", \"respondent_id\"]\n",
    "            r_df = r_df.loc[:, columns]\n",
    "            good_dfs[r] = r_df\n",
    "\n",
    "        else:\n",
    "            print(r, len(r_df))\n",
    "            error_dfs[r] = r_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIFVTZb8r2Tc"
   },
   "source": [
    "From the code above it looks like ~120 BAs have data clean enough that I'm able to extract the correct number of hours. \n",
    "\n",
    "\\#225 has 2 days with demand in hour25 (daylight savings) but only one of the years has an hour in March with 0 demand. I suppose I could just remove an hour from that day?\n",
    "\n",
    "\\# 311 has demand in hour25 every day (or just about). Interestingly, hour01 and hour02 seem to always have the same values. Maybe just drop hour01 and shift everything over?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZyhiKg9EbOW"
   },
   "source": [
    "## Check the demand data for anomalies\n",
    "\n",
    "The anomaly checking functions and parameter values below are all from [a notebook](https://github.com/truggles/EIA_Cleaned_Hourly_Electricity_Demand_Code) by Tyler Ruggles, which he developed to screen [hourly demand data from EIA-931](https://www.eia.gov/realtime_grid/#/status?end=20200325T07). I've modified some functions to speed them up. EIA's hourly data only goes back to mid-2015, which is why I'm using FERC 714. Fortunately it looks like the FERC data — once properly extracted — doesn't have many of the anomalies that Tyler found in the EIA data. Tyler also [developed a method](https://github.com/truggles/EIA_Cleaned_Hourly_Electricity_Demand_Code/blob/master/MICE_step.Rmd) to impute missing or anomalous data but at first glance it looks like the FERC demand data can largely be used as-is without too many issues. Let me know if you disagree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXjdiqakbxcQ"
   },
   "outputs": [],
   "source": [
    "short_hour_window = 24 # 48 hour moving median (M_{t,48hr})\n",
    "iqr_hours = 24*5 # width in hours of IQR values of relative deviations from diurnal cycle template (IQR_{dem,t})\n",
    "nDays = 10 # Used for normalized hourly demand template (h_{t,diurnal}) and 480 hour moving median (M_{t,480hr})\n",
    "global_dem_cut = 10 # threshold selection for global demand filter\n",
    "local_dem_cut_up = 3.5 # upwards threshold for local demand filter\n",
    "local_dem_cut_down = 2.5 # downwards threshold for local demand filter\n",
    "delta_multiplier = 2 # selection threshold for double-sided delta filter\n",
    "delta_single_multiplier = 5 # selection threshold for single-sided delta filter\n",
    "rel_multiplier = 15 # other selection threshold for single-sided delta filter\n",
    "anomalous_regions_width = 24 # width in hours of anomalous region filter\n",
    "anomalous_pct = .85 # required pct of good data in anomalous region filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "jRQ3Zq5nEeVf",
    "outputId": "b827b69f-bb14-47f2-d489-154ccfe1fa30"
   },
   "outputs": [],
   "source": [
    "name_df_list = Parallel(n_jobs=-1, verbose=10)(delayed(screen_timeseries)(\n",
    "    name=name, \n",
    "    df=df, \n",
    "    short_hour_window=short_hour_window,\n",
    "    iqr_hours=iqr_hours,\n",
    "    nDays=nDays,\n",
    "    global_dem_cut=global_dem_cut,\n",
    "    local_dem_cut_up=local_dem_cut_up,\n",
    "    local_dem_cut_down=local_dem_cut_down,\n",
    "    delta_multiplier=delta_multiplier,\n",
    "    delta_single_multiplier=delta_single_multiplier,\n",
    "    rel_multiplier=rel_multiplier,\n",
    "    anomalous_regions_width=rel_multiplier,\n",
    "    anomalous_pct=anomalous_pct\n",
    "    ) for name, df in good_dfs.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "5WcZMeOTEkaf",
    "outputId": "e8f8e35f-6c81-4be0-a134-f0b4437c7d99"
   },
   "outputs": [],
   "source": [
    "anomaly_dict = {name: df for name, df in name_df_list}\n",
    "summary_df = make_anomaly_summary(anomaly_dict)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "D-XSSs6AE18r",
    "outputId": "759d5cb4-8f81-48ba-a128-e9aa6cd22996"
   },
   "outputs": [],
   "source": [
    "summary_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFDpZLG6E4cH"
   },
   "source": [
    "## Matching FERC respondents to EIA-816 utilities\n",
    "\n",
    "The SI of [Auffhammer et al](https://www.pnas.org/content/pnas/suppl/2017/02/01/1613193114.DCSupplemental/pnas.1613193114.sapp.pdf) describes how they derived geographic coverage of FERC respondents using EIA-861 and the crosswalk between FERC respondents and EIA utilities/BAs. I'm including a little code to start exploring that here.\n",
    "\n",
    "This is very exploratory! I'm trying to match codes between FERC and 861 but haven't figured out yet if they should be matched to EIA BAs or Utilities. At a minimum it looks like there are 84 FERC respondents that don't have a match in either category of the 2012 861.\n",
    "\n",
    "A few extra resources that might be helpful:\n",
    "- SPP [historical hourly load](https://marketplace.spp.org/pages/hourly-load#) back through 2011. The company acronyms (at least for 2011/2012 data) are described in [this document](https://www.nerc.com/pa/rrm/Resources/Monitoring_and_Situational_Awareness_Conference2/2%20Testing%20Your%20Sensitivity%20To%20Loss%20of%20Data_T%20Miller.pdf).\n",
    "- MISO has [archivled historical hourly load](https://www.misoenergy.org/markets-and-operations/real-time--market-data/market-reports/market-report-archives/#nt=%2FMarketReportType%3ASummary%2FMarketReportName%3AArchived%20Historical%20Regional%20Forecast%20and%20Actual%20Load%20%20(zip)&t=10&p=0&s=MarketReportPublished&sd=desc) for 2007-2014 and [historical hourly load](https://www.misoenergy.org/markets-and-operations/real-time--market-data/market-reports/#nt=%2FMarketReportType%3ASummary%2FMarketReportName%3AHistorical%20Regional%20Forecast%20and%20Actual%20Load%20(xls)&t=10&p=0&s=MarketReportPublished&sd=desc) back through 2013 for Central, East, and West regions.\n",
    "- ERCOT has [historical hourly load](http://www.ercot.com/gridinfo/load/load_hist/) in each of the 8 weather zones. (NOTE: the IPM region ERC_PHDL has no demand, so all weather zones are split into ERC_WEST and ERC_REST)\n",
    "- PJM requires a login and API key to access their [historical metered data](https://dataminer2.pjm.com/feed/hrl_load_metered/definition) through dataminer (back to 1993).\n",
    "- NYISO has hourly load by zone (which look like they group well into IPM regions) back through 2001. I need to figure out the difference between [real time actual load](http://mis.nyiso.com/public/P-58Blist.htm) and [integrated real time actual load](http://mis.nyiso.com/public/P-58Clist.htm).\n",
    "- ISONE has [hourly load by zone](https://www.iso-ne.com/isoexpress/web/reports/load-and-demand/-/tree/zone-info) back through 2011. Unfortunately, from a quick glance at 2011 it looks like while there are only 8760 hourly demand values the 2am on March 13 (DST) has demand of 0. Oh, but the 2am value on Nov. 6 is twice the surrounding hours, so they appear to add two hours together on one line.... (Need to remember this as something other places might do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PN2ouEdRE3-1"
   },
   "outputs": [],
   "source": [
    "# Download the 2012 EIA-861 data to a temp folder\n",
    "url = 'https://www.eia.gov/electricity/data/eia861/archive/zip/f8612012.zip'\n",
    "save_folder = cwd / \"EIA861\"\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "urllib.request.urlretrieve(url, save_folder / 'f8612012.zip')\n",
    "### Unzip it\n",
    "data_path = save_folder / \"f8612012\"\n",
    "with zipfile.ZipFile(save_folder / 'f8612012.zip', 'r') as zfile:\n",
    "    zfile.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "OIsdXi04E9dr",
    "outputId": "7fd86d1b-8fe3-4a2a-8697-008f8f09366a"
   },
   "outputs": [],
   "source": [
    "ferc_eia_map = pd.read_csv(cwd / \"FERC\" / \"form714-database\" / \"Respondent IDs.csv\", index_col=0)\n",
    "\n",
    "# Only keep the respondents that are in the years of data we're looking at\n",
    "ferc_eia_map = ferc_eia_map.loc[respondents, :]\n",
    "ferc_eia_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sf_Ov6x6FBrW"
   },
   "outputs": [],
   "source": [
    "# 307 is PacifiCorp - Part II Sch 2 (East & West combined), which looks to be\n",
    "# EIA utility number 14354\n",
    "summary_df[\"eia_code\"] = summary_df[\"name\"].map(ferc_eia_map[\"eia_code\"])\n",
    "summary_df.query(\"~(eia_code > 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "WFPHvT4uFH-c",
    "outputId": "3841e718-4e8d-4858-f578-f74289489b8c"
   },
   "outputs": [],
   "source": [
    "eia8612012_territory = pd.read_excel(cwd / \"EIA861\" / \"f8612012\" / \"service_territory_2012.xls\")\n",
    "eia8612012_territory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On7uQOayFM_a"
   },
   "outputs": [],
   "source": [
    "# Document which ferc respondents can match with a utility. These utilities have\n",
    "# a list of all the counties they are active in.\n",
    "ferc_eia_map[\"utility_match\"] = False\n",
    "ferc_eia_map.loc[\n",
    "    ferc_eia_map.eia_code.isin(eia8612012_territory['Utility Number']),\n",
    "    \"utility_match\"\n",
    "] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "sjd_p3v7FOpP",
    "outputId": "485654c2-d2bd-4fad-c7ac-675159becea8"
   },
   "outputs": [],
   "source": [
    "eia8612012_bas = pd.read_excel(cwd / \"EIA861\" / \"f8612012\" / \"balancing_authority_2012.xls\")\n",
    "eia8612012_bas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdwzToM0FTo_"
   },
   "outputs": [],
   "source": [
    "ferc_eia_map[\"ba_match\"] = False\n",
    "ferc_eia_map.loc[\n",
    "    ferc_eia_map.eia_code.isin(eia8612012_bas['BA Code']),\n",
    "    \"ba_match\"\n",
    "] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vs0kIlCKoIM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kg191UxKqJx"
   },
   "source": [
    "## Examine which entities we can match to BAs and utilities\n",
    "\n",
    "It looks like some of the respondents are only BAs, some are only utilities, and some are both. Will need to figure out if any of the utilities that are not a BA are within a BA territory - are there cases where demand from one respondent is a subset of another respondent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "771mtwQhKUDj",
    "outputId": "58ffb2c7-77c2-4265-c82e-65be5915dcf0"
   },
   "outputs": [],
   "source": [
    "ferc_eia_map.query(\"ba_match==True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PGcpGgxZFVJW",
    "outputId": "42ec3fed-04dd-4e0b-bf20-9b0eb0fb8df3"
   },
   "outputs": [],
   "source": [
    "ferc_eia_map.query(\"utility_match==False & ba_match==False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pCiP34U6FXNA",
    "outputId": "33f7149c-eaf5-4ce9-8ed1-ff8b86392272"
   },
   "outputs": [],
   "source": [
    "# Some entities are both a BA and a utility\n",
    "ferc_eia_map.query(\"utility_match==True & ba_match==True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "colab_type": "code",
    "id": "OLdiLqsUFkCS",
    "outputId": "756d3525-559f-45cf-c061-dabc7bb0e81b"
   },
   "outputs": [],
   "source": [
    "ferc_eia_map.query(\"utility_match==True & ba_match==False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6layIR6KJ_C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "FERC714_exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
